# Frequentist and Bayesian via Measure Theory
**Question:** Why do we care about our prior belief $p(\theta_{i-1})$ if we have a data point $x_{i-1}, y_{i-1}$. Why should we not just build our whole belief system only using current knowledge.

**Answer:** Amazing question. This question proves that you understand the battle between frequentist and Bayesian argument. Removing your prior belief says that the seen world has the ultimate unchangable truth. The world has an underlying fixed law. The law is governed by some unknown parameters $\{\theta_{i}\}_{i\in{I\subseteq\mathbb{R}}}$. Einstein called those parameters `hidden variables`, which was his famous battle with `Werner Heisenberg` for new science called `Quantum Mechanics` in 1920s. Considering that the parameters were created by God, we do not dare to put error in God's work, which means they are iternal and unchangable, no uncertainty is there. We, mortal human beings are no one to change them or having a prior judgement over God's creation. We should rather wait until we observe God's truth when God will reveal them to us. As a servant of God we should never attempt to guess His mind. We should only be satisfied with what we are given from Heaven and do analysis on top of that. This is exactly what we call the `frequentist` view. So, probability is nothing but the decision of long term observations. Something didn't happen - we do not talk about any probability there. This seems a quite reseanable. Now, think about the question - What is the probability that Donald Trump will win the 2020 election against democrats? There is one sample of him winning against democrats in 2016. So, it must be 100%. But that was not the case, **he lost**. So, the argument didn't work. Because, saying 100% is equivalent to `overfitting` by `maximizing the likelihood`, $p(y_{i-1} | x_{i-1}, \theta_{i-1})$ of the underlying truth (`model parameters`) for the observed data to be true. The dataset is just not big enough to do such `maximization of the likelihood`. How about we have the idea that the world is going to a progressive direction? How about talking about Trump's ignorance to `Black lives matter` protest? How about his ignorance to the `environment`. How about doing some of the aspects of his personality? We can do those. But can we do those mathematically? These are very hard to quantify for calculation. The proposal here is just assigning some prior beliefs before any calculation? We will call it the `prior distribution`, $p(\theta_{i-1})$. Then we will take a data point of Trump winning 2016 election. If we are correct, then we celebrate our correct calculation. If not, not a very big deal, because we can be sure that we are not putting importance only on 2016 election, which is good and more favorable in this small dataset case. We also have to take care of the situation whether Donald Trump will go for the election at all, we call it the `denominator` term $p(x_{i-1}, y_{i-1})$ term. All these information can be combined using `Bayes formula` to obtain the result which is our target for updating our knowledge of the underlying parameter $p(\theta_{i} | \theta_{i-1}, x_{i-1}, y_{i-1})$, we call it the `posterior distribution`. The next step would be using this updated distribution as our next `prior distribution` to calculate the next `posterior distribution`. This is the way Bayesian thinking goes in order to gradually and recursively understand the underlying law of nature with the advent of new data using Bayes formula,
\begin{equation}
p(\theta_{i+1} | \theta_i, x_i, y_i) = \frac{p(y_i | x_i, \theta_i)p(\theta_i)}{p(x_i, y_i)}
\end{equation}
Recursion goes like the following
\begin{equation}
{p(\theta_3 | \theta_2, x_2, y_2)} \propto {p(y_2 | x_2, \theta_2)} \underbrace{{p(y_1 | x_1, \theta_1)} \underbrace{{p(y_0 | x_0, \theta_0)} {p(\theta_0)}}_{{p(\theta_1)}} }_{{p(\theta_2)}}
\end{equation}
Finally, the `predictive distribution`, which is obtained by marginalizing over the model parameters 
\begin{equation}
{p(y_i | x_i)} = \int {p(y_i | \textbf{w}, x_i)} {p(\textbf{w})} d\textbf{w}
\end{equation}

See the notebook file for more detail.
